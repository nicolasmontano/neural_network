{"cells":[{"cell_type":"markdown","id":"9132af12","metadata":{"id":"9132af12"},"source":["# Implementation of Email Auto-completion"]},{"cell_type":"markdown","id":"0e34d074","metadata":{"id":"0e34d074"},"source":["## Objectives"]},{"cell_type":"markdown","id":"6610f72e","metadata":{"id":"6610f72e"},"source":["* Use the previously cleanded sentences from email dataset\n","* Tokenization of the sentences\n","* Padding of the sentences\n","* Create the network architecture\n","* Train the model\n","* Get predictions from the trained model"]},{"cell_type":"markdown","id":"69543679","metadata":{"id":"69543679"},"source":["## The problem:\n","\n","Suppose that we are working in Globomantics which is one of the most popular email applications in the world. To improve user experience, you want to build an intelligent system which will provide auto-completion suggestions to users during email compose. We want to be sure that the suggestions are relevant and useful to the users so that the user experience enhances."]},{"cell_type":"markdown","id":"729dafc2","metadata":{"id":"729dafc2"},"source":["## Dataset\n","\n","We'll be using the Enron email dataset which is one of the most popular email datasets. The dataset can be downloaded from [here](https://www.kaggle.com/code/abhaytomar/starter-the-enron-email-dataset-8c90cc3c-1/data).\n","\n","This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation. More information about this dataset can be found [here](https://www.cs.cmu.edu/~enron/)."]},{"cell_type":"markdown","id":"b98a734b","metadata":{"id":"b98a734b"},"source":["## Import Libraries and Load the Cleaned Sentences"]},{"cell_type":"code","execution_count":null,"id":"b4b238fd","metadata":{"id":"b4b238fd"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":1,"id":"92527256","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"92527256","executionInfo":{"status":"error","timestamp":1723925377034,"user_tz":-60,"elapsed":7,"user":{"displayName":"Nicolas Monta√±o","userId":"09446804150664905730"}},"outputId":"e563aa52-6022-471c-a4cd-22d128453026"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-14cfa8217a32>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentence_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentences.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentence_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["sentence_df = pd.read_csv('sentences.csv')\n","sentence_df = sentence_df.dropna()\n","sentence_df.head()"]},{"cell_type":"code","execution_count":null,"id":"ddb8bdeb","metadata":{"id":"ddb8bdeb","outputId":"767b91b2-26f5-432f-8743-bbc14d137ea5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of sentence:  152831\n"]}],"source":["sentences = sentence_df.sentence.values\n","print(\"Total number of sentence: \", len(sentences))"]},{"cell_type":"code","execution_count":null,"id":"3fb41a2f","metadata":{"id":"3fb41a2f","outputId":"c4547190-d8ae-42d6-f6df-78d077f1d9fb"},"outputs":[{"data":{"text/plain":["array(['here is our forecast',\n","       'traveling to have a business meeting takes the fun out of the trip',\n","       'especially if you have to prepare a presentation',\n","       'i would suggest holding the business plan meetings here then take a trip without any formal business meetings',\n","       'i would even try and get some honest opinions on whether a trip is even desired or necessary',\n","       'too often the presenter speaks and the others are quiet just waiting for their turn',\n","       'the meetings might be better if held in a round table discussion format',\n","       'play golf and rent a ski boat and jet ski is',\n","       'flying somewhere takes too much time',\n","       'plus your thoughts on any changes that need to be made'],\n","      dtype=object)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["sentences[0:10]"]},{"cell_type":"code","execution_count":null,"id":"2f74f507","metadata":{"id":"2f74f507"},"outputs":[],"source":["sentences = sentences[:30000]"]},{"cell_type":"markdown","id":"58d45479","metadata":{"id":"58d45479"},"source":["## Tokenization of the sentences"]},{"cell_type":"markdown","id":"decab1dc","metadata":{"id":"decab1dc"},"source":["We'll use keras tokenizer class and its methods to perform tokenization, create vocabulary and the word to number mapping. To know more about tokenizer class, please consult this [link](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."]},{"cell_type":"code","execution_count":null,"id":"3de0eb52","metadata":{"id":"3de0eb52"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"id":"f829d607","metadata":{"id":"f829d607"},"outputs":[],"source":["test_tokenizer = Tokenizer()"]},{"cell_type":"code","execution_count":null,"id":"d1bdfdc4","metadata":{"id":"d1bdfdc4"},"outputs":[],"source":["test_sentences = ['here is our forecast',\n","                  'especially if you have to prepare a presentation']"]},{"cell_type":"code","execution_count":null,"id":"3b6fc0cd","metadata":{"id":"3b6fc0cd"},"outputs":[],"source":["test_tokenizer.fit_on_texts(test_sentences)"]},{"cell_type":"code","execution_count":null,"id":"16b631bc","metadata":{"id":"16b631bc","outputId":"b7d65147-b78d-4342-9be8-aed093e90bd0"},"outputs":[{"data":{"text/plain":["{'here': 1,\n"," 'is': 2,\n"," 'our': 3,\n"," 'forecast': 4,\n"," 'especially': 5,\n"," 'if': 6,\n"," 'you': 7,\n"," 'have': 8,\n"," 'to': 9,\n"," 'prepare': 10,\n"," 'a': 11,\n"," 'presentation': 12}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["test_tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"id":"4dbbfd97","metadata":{"id":"4dbbfd97","outputId":"f9194abb-7aad-41df-d5ea-b57552d361cc"},"outputs":[{"data":{"text/plain":["{1: 'here',\n"," 2: 'is',\n"," 3: 'our',\n"," 4: 'forecast',\n"," 5: 'especially',\n"," 6: 'if',\n"," 7: 'you',\n"," 8: 'have',\n"," 9: 'to',\n"," 10: 'prepare',\n"," 11: 'a',\n"," 12: 'presentation'}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["test_tokenizer.index_word"]},{"cell_type":"code","execution_count":null,"id":"fe022b37","metadata":{"id":"fe022b37","outputId":"a01b1287-0cd9-4ae2-8b65-4cb1efa2bf6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 7, 8, 3, 12]\n"]}],"source":["test_sentence = \"here you have our presentation\"\n","test_token_list = test_tokenizer.texts_to_sequences([test_sentence])[0]\n","print(test_token_list)"]},{"cell_type":"code","execution_count":null,"id":"942a6b71","metadata":{"id":"942a6b71","outputId":"9d3bba04-ad2d-4f7b-fcd8-d47044bfc884"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1, 7], [1, 7, 8], [1, 7, 8, 3], [1, 7, 8, 3, 12]]\n"]}],"source":["n_grams = []\n","for i in range(1, len(test_token_list)):\n","    n_gram = test_token_list[:i+1]\n","    n_grams.append(n_gram)\n","print(n_grams)"]},{"cell_type":"code","execution_count":null,"id":"1cca6d94","metadata":{"id":"1cca6d94"},"outputs":[],"source":["tokenizer = Tokenizer()\n","def convertSentencesIntoSeqOfTokens(sentences):\n","    tokenizer.fit_on_texts(sentences)\n","    total_words_in_vocab = len(tokenizer.word_index) + 1\n","\n","    input_sequences = []\n","    for sentence in sentences:\n","        seq_of_tokens = tokenizer.texts_to_sequences([sentence])[0]\n","        for i in range(1, len(seq_of_tokens)):\n","            n_gram = seq_of_tokens[:i+1]\n","            input_sequences.append(n_gram)\n","    return input_sequences, total_words_in_vocab"]},{"cell_type":"code","execution_count":null,"id":"f959dfcd","metadata":{"id":"f959dfcd","outputId":"fd9164f3-735e-433b-f2e5-625590d3dd4b"},"outputs":[{"data":{"text/plain":["[[98, 4],\n"," [98, 4, 41],\n"," [98, 4, 41, 1828],\n"," [2263, 2],\n"," [2263, 2, 17],\n"," [2263, 2, 17, 5],\n"," [2263, 2, 17, 5, 111],\n"," [2263, 2, 17, 5, 111, 114],\n"," [2263, 2, 17, 5, 111, 114, 758],\n"," [2263, 2, 17, 5, 111, 114, 758, 1]]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["input_sequences, total_words_in_vocab = convertSentencesIntoSeqOfTokens(sentences)\n","input_sequences[:10]"]},{"cell_type":"markdown","id":"6dfd7b89","metadata":{"id":"6dfd7b89"},"source":["## Handle variable sentence lengths by padding"]},{"cell_type":"markdown","id":"afe8b9b1","metadata":{"id":"afe8b9b1"},"source":["We'll use the Keras \"pad_sequences\" function to pad smaller sequences. To know more about this function, please go through this [link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences)"]},{"cell_type":"code","execution_count":null,"id":"ca33bbe8","metadata":{"id":"ca33bbe8"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"id":"93ba1827","metadata":{"id":"93ba1827"},"outputs":[],"source":["test_sequences = [[2025, 2], [2025, 2, 16], [2025, 2, 16, 6],\n","                  [2025, 2, 16, 6, 135], [2025, 2, 16, 6, 135, 119]]"]},{"cell_type":"code","execution_count":null,"id":"bd2f2539","metadata":{"id":"bd2f2539","outputId":"da74717a-d136-4c4e-95bd-47d51830f4f9"},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0,    0, 2025,    2],\n","       [   0,    0,    0, 2025,    2,   16],\n","       [   0,    0, 2025,    2,   16,    6],\n","       [   0, 2025,    2,   16,    6,  135],\n","       [2025,    2,   16,    6,  135,  119]], dtype=int32)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["pad_sequences(test_sequences, maxlen=6, padding='pre')"]},{"cell_type":"code","execution_count":null,"id":"8c2b812a","metadata":{"id":"8c2b812a"},"outputs":[],"source":["def generateSameLengthSentencesByPadding(sequences):\n","    # Find length of the longest sequence\n","    max_seq_len = max([len(x) for x in sequences])\n","\n","    # Pad the senquences\n","    padded_sequences = np.array(pad_sequences(sequences, maxlen=max_seq_len, padding='pre'))\n","\n","    # Return padded sequences and the max length\n","    return padded_sequences, max_seq_len"]},{"cell_type":"code","execution_count":null,"id":"465aa747","metadata":{"id":"465aa747","outputId":"e52639ba-0f1e-4880-9956-e83d800fe232"},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,   98,    4],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,   98,    4,   41],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,   98,    4,   41, 1828],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0, 2263,    2],\n","       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0, 2263,    2,   17]], dtype=int32)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["padded_sequences, max_seq_len = generateSameLengthSentencesByPadding(input_sequences)\n","padded_sequences[:5]"]},{"cell_type":"markdown","id":"078b37f7","metadata":{"id":"078b37f7"},"source":["## Generate predictors and labels for training"]},{"cell_type":"markdown","id":"be045f48","metadata":{"id":"be045f48"},"source":["We are importing keras utils here, this will be needed to convert the labels to one-hot encoded vectors. We'll use the the function \"to_categorical\" from this library to do this. To know more about this function, please check out this [link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)."]},{"cell_type":"code","execution_count":null,"id":"d79a5c4c","metadata":{"id":"d79a5c4c"},"outputs":[],"source":["import tensorflow.keras.utils as ku"]},{"cell_type":"code","execution_count":null,"id":"93158ed5","metadata":{"id":"93158ed5"},"outputs":[],"source":["test_padded_sequences = np.array([[0, 0, 0, 12, 16, 32],\n","                                  [0, 0, 8, 15, 17, 41]])"]},{"cell_type":"markdown","id":"85abc484","metadata":{"id":"85abc484"},"source":["We'll use array slicing techniques to retrieve the inputs and the labels. To know more about how indexing into a numpy array is done, please go through the following resources. [link1](https://towardsdatascience.com/slicing-numpy-arrays-like-a-ninja-e4910670ceb0), [link2](https://www.tutorialspoint.com/numpy/numpy_indexing_and_slicing.htm)"]},{"cell_type":"code","execution_count":null,"id":"bbe50091","metadata":{"id":"bbe50091","outputId":"7e764084-ff4b-419a-c07a-cd9cd33686a1"},"outputs":[{"data":{"text/plain":["array([[ 0,  0,  0, 12, 16],\n","       [ 0,  0,  8, 15, 17]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_padded_sequences[:,:-1]"]},{"cell_type":"code","execution_count":null,"id":"b1ccd7e9","metadata":{"id":"b1ccd7e9","outputId":"805fba9c-c1e1-4dbc-8ddf-a8288c73a307"},"outputs":[{"data":{"text/plain":["array([32, 41])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["test_padded_sequences[:,-1]"]},{"cell_type":"code","execution_count":null,"id":"2f9bc390","metadata":{"id":"2f9bc390"},"outputs":[],"source":["def generatePredictorsAndLabels(padded_sequences):\n","    inputs, label = padded_sequences[:,:-1], padded_sequences[:,-1]\n","    label = ku.to_categorical(label, num_classes = total_words_in_vocab)\n","    return inputs, label"]},{"cell_type":"code","execution_count":null,"id":"c3bf8c7a","metadata":{"id":"c3bf8c7a"},"outputs":[],"source":["inputs, label = generatePredictorsAndLabels(padded_sequences)"]},{"cell_type":"markdown","id":"3ccafba3","metadata":{"id":"3ccafba3"},"source":["## Create and train the model"]},{"cell_type":"markdown","id":"7da87dd7","metadata":{"id":"7da87dd7"},"source":["Import Sequential model from keras and Embedding, LSTM, Dense and Dropout layers from Keras. To know more about them, consult these links. [sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential), [embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n","\n","To know more about regularization, overfitting and dropout strategy, please consult this [link](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)."]},{"cell_type":"code","execution_count":null,"id":"9f3d5e2e","metadata":{"id":"9f3d5e2e"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"]},{"cell_type":"code","execution_count":null,"id":"4a67e545","metadata":{"id":"4a67e545"},"outputs":[],"source":["input_length = max_seq_len - 1"]},{"cell_type":"code","execution_count":null,"id":"398b2e6f","metadata":{"id":"398b2e6f","outputId":"56aaaa4b-23a4-4302-a07a-c4de19cd00ac"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-21 01:55:02.442051: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["model = Sequential()\n","model.add(Embedding(total_words_in_vocab, 10, input_length=input_length))\n","model.add(LSTM(100))\n","model.add(Dropout(0.1))\n","model.add(Dense(total_words_in_vocab, activation='softmax'))"]},{"cell_type":"markdown","id":"f3b2bfaa","metadata":{"id":"f3b2bfaa"},"source":["Compile the model by specifying the loss function and the optimizer which the model will use during training. To know more about different loss functions and optimizers, go through these links. [link1](https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c), [link2](https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a), [link3](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"]},{"cell_type":"code","execution_count":null,"id":"2ed028b3","metadata":{"id":"2ed028b3","outputId":"b2a4e7e4-655c-48cf-efe9-27f2bdea6967"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 27, 10)            160270    \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               44400     \n","                                                                 \n"," dropout (Dropout)           (None, 100)               0         \n","                                                                 \n"," dense (Dense)               (None, 16027)             1618727   \n","                                                                 \n","=================================================================\n","Total params: 1,823,397\n","Trainable params: 1,823,397\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"id":"793f906f","metadata":{"id":"793f906f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9b06faa3","metadata":{"id":"9b06faa3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f39f2f38","metadata":{"id":"f39f2f38"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fdaa5870","metadata":{"id":"fdaa5870"},"outputs":[],"source":["model.fit(inputs, label, epochs=100)"]},{"cell_type":"markdown","id":"83bc2b58","metadata":{"id":"83bc2b58"},"source":["Once you are done with the training, you can save your model. To know more about this, follow this [link](https://www.tensorflow.org/guide/keras/save_and_serialize)"]},{"cell_type":"code","execution_count":null,"id":"73f4c676","metadata":{"id":"73f4c676"},"outputs":[],"source":["model.save('lstm_text_autocomplete')"]},{"cell_type":"code","execution_count":null,"id":"b30cd7a6","metadata":{"id":"b30cd7a6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a5a36d54","metadata":{"id":"a5a36d54"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d07e2fa4","metadata":{"id":"d07e2fa4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"01183ecc","metadata":{"id":"01183ecc"},"source":["## Generate autocomplete suggestions using the trained model"]},{"cell_type":"code","execution_count":null,"id":"ae90ba35","metadata":{"id":"ae90ba35"},"outputs":[],"source":["from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"id":"fd8905fe","metadata":{"id":"fd8905fe"},"outputs":[],"source":["model = load_model('lstm_text_autocomplete')"]},{"cell_type":"code","execution_count":null,"id":"d92697ee","metadata":{"id":"d92697ee"},"outputs":[],"source":["def generate_autocomplete_suggestions(seed_sentence, no_of_next_words,\n","                                      model, max_sequence_len):\n","    for _ in range(no_of_next_words):\n","        sequence = tokenizer.texts_to_sequences([seed_sentence])[0]\n","\n","        padded_sequence = pad_sequences([sequence],\n","                                        maxlen=max_seq_len-1,\n","                                        padding='pre')\n","\n","        predictions = model.predict(padded_sequence, verbose=0)\n","\n","        predicted_label = np.argmax(predictions, axis=1)[0]\n","\n","        next_word = tokenizer.index_word[predicted_label]\n","\n","        seed_sentence += \" \"+ next_word\n","\n","    return seed_sentence"]},{"cell_type":"code","execution_count":null,"id":"5175cb4b","metadata":{"id":"5175cb4b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"6cd4e0e7","metadata":{"id":"6cd4e0e7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ec675797","metadata":{"id":"ec675797"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"49330b01","metadata":{"id":"49330b01","outputId":"04d37335-0946-4f22-d4e4-a199101a9962"},"outputs":[{"name":"stdout","output_type":"stream","text":["In response to your earlier email and will be sent to you and the other group\n","I am happy to get a little bit of the ball at the same\n","What is the status of the project\n","Here is the data of the company\n","Thank you very much for your help and\n","I got your email to me and if you have any questions or concerns about this process please let me know\n"]}],"source":["print (generate_autocomplete_suggestions(\"In response to your earlier email\", 10,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"I am happy to\", 10,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"What is the status\", 3,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"Here is the data\", 3,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"Thank you very much\", 4,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"I got your email\", 17,\n","                                         model, max_seq_len))"]},{"cell_type":"code","execution_count":null,"id":"00e275ed","metadata":{"id":"00e275ed"},"outputs":[],"source":["\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}