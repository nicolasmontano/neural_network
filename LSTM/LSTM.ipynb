{"cells":[{"cell_type":"markdown","metadata":{"id":"yY0xTbVY4f_2"},"source":["# Data Preparation for Implementation of Email Auto-completion"]},{"cell_type":"markdown","metadata":{"id":"uWJzQl5e4f_3"},"source":["## Objectives"]},{"cell_type":"markdown","metadata":{"id":"C3O-iDmy4f_3"},"source":["* Explore Enron email dataset\n","* Extract email messages from raw email data\n","* Clean email messages\n","* Preprocessing of email messages to make them ready for training"]},{"cell_type":"markdown","metadata":{"id":"17H2i0sa4f_4"},"source":["## The problem:\n","\n","Suppose that we are working in Globomantics which is one of the most popular email applications in the world. To improve user experience, you want to build an intelligent system which will provide auto-completion suggestions to users during email compose. We want to be sure that the suggestions are relevant and useful to the users so that the user experience enhances."]},{"cell_type":"markdown","metadata":{"id":"VymjKNKa4f_4"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"U-nI97Zb4f_4"},"source":["We'll be using the Enron email dataset which is one of the most popular email datasets. The dataset can be downloaded from [here](https://www.kaggle.com/code/abhaytomar/starter-the-enron-email-dataset-8c90cc3c-1/data).\n","\n","This dataset was collected and prepared by the CALO Project (A Cognitive Assistant that Learns and Organizes). It contains data from about 150 users, mostly senior management of Enron. The corpus contains a total of about 0.5M messages. This data was originally made public, and posted to the web, by the Federal Energy Regulatory Commission during its investigation. More information about this dataset can be found [here](https://www.cs.cmu.edu/~enron/)."]},{"cell_type":"markdown","metadata":{"id":"69qpZQQL4f_4"},"source":["## Explore the Dataset"]},{"cell_type":"markdown","metadata":{"id":"xNohAQS54f_4"},"source":["Import basic libraries like pandas and numpy. We are also importing the [email](https://docs.python.org/3/library/email.html) package which we'll use for extracting email messages from raw email data. We are also importing the python package to use [regular expressions](https://docs.python.org/3/library/re.html). This will be necessary to preprocess the text data."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Cmnc38BF4f_5","executionInfo":{"status":"ok","timestamp":1723930859561,"user_tz":-60,"elapsed":271,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import email\n","import re"]},{"cell_type":"code","source":["!kaggle datasets download -d wcukierski/enron-email-dataset\n","!unzip enron-email-dataset.zip"],"metadata":{"id":"cIsN7hVk6An6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GKU6Ui0h4f_5"},"source":["The Enron email dataset is already downloaded from [here]((https://www.kaggle.com/code/abhaytomar/starter-the-enron-email-dataset-8c90cc3c-1/data)) and saved as a CSV file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sw3Ydfgp4f_6"},"outputs":[],"source":["emails_raw = pd.read_csv('emails.csv')\n","emails_raw.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgwanlgS4f_6"},"outputs":[],"source":["emails_raw.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siLJim6B4f_6"},"outputs":[],"source":["print(emails_raw['message'].iloc[1])"]},{"cell_type":"markdown","metadata":{"id":"hRVdbrGz4f_6"},"source":["## Extract Email Message from Raw Email Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VUHT16W4f_6"},"outputs":[],"source":["sample_email = emails_raw['message'].iloc[1]\n","print(sample_email)"]},{"cell_type":"markdown","metadata":{"id":"gX9EU31V4f_7"},"source":["We'll create an email object using [message_from_string](https://docs.python.org/3/library/email.parser.html). Then, we'll traverse the email object and collect all the plaintext components using the functions [walk](https://docs.python.org/3/library/email.message.html),[get_content_type](https://docs.python.org/3/library/email.message.html) and [get_payload](https://docs.python.org/3/library/email.compat32-message.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p1LwFaeU4f_7"},"outputs":[],"source":["sample_email_object = email.message_from_string(sample_email)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gC8d2Ayo4f_7"},"outputs":[],"source":["plain_text_parts = []\n","for part in sample_email_object.walk():\n","    # print(part)\n","    if part.get_content_type() == 'text/plain':\n","        plain_text_parts.append(part.get_payload())\n","sample_content = ''.join(plain_text_parts)\n","print(sample_content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wxYwId2S4f_7"},"outputs":[],"source":["def extract_plaintext_content(raw_email):\n","    # create an email object\n","    email_object = email.message_from_string(raw_email)\n","\n","    # create an empty list to store all the plaintext components\n","    plain_text_parts = []\n","\n","    # traverse over different parts of the email object and collect the plaintext parts\n","    for part in email_object.walk():\n","\n","        # check if the part is of type plaintext\n","        if part.get_content_type() == 'text/plain':\n","\n","            # get payload if the type is plaintext\n","            plain_text_parts.append(part.get_payload())\n","\n","    # concatenate the plaintext parts and return\n","    return ''.join(plain_text_parts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7ohUGgR4f_7"},"outputs":[],"source":["# Apply the function over all the raw emails to extract the email message\n","emails = pd.DataFrame()\n","emails['content'] = [extract_plaintext_content(i) for i in emails_raw['message']]\n","emails.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FPNrdlBt4f_7"},"outputs":[],"source":["print(emails[\"content\"].iloc[1])"]},{"cell_type":"markdown","metadata":{"id":"T2-vO5in4f_8"},"source":["## Clean the Dataset by Discarding Unnecessary Emails"]},{"cell_type":"markdown","metadata":{"id":"cA7v3B5W4f_8"},"source":["Remove all outlook migration emails"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTx9Wkyv4f_8"},"outputs":[],"source":["print(emails[\"content\"].iloc[61])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YCZiKXj4f_8"},"outputs":[],"source":["outlook_migration_emails = emails[emails['content'].str.contains('Outlook Migration Team@ENRON')]\n","print(\"No of outlook migration emails\", outlook_migration_emails.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuJLqRnl4f_8"},"outputs":[],"source":["emails = emails[~emails['content'].str.contains('Outlook Migration Team@ENRON')]"]},{"cell_type":"markdown","metadata":{"id":"l2csNq1J4f_8"},"source":["Remove all the sales emails"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZaPehh24f_8"},"outputs":[],"source":["trade_count_emails = emails[emails['content'].str.contains('Trade Counts and Volume')]\n","print(trade_count_emails[\"content\"].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFyjIkIl4f_8"},"outputs":[],"source":["emails = emails[~emails['content'].str.contains('Trade Counts and Volume')]"]},{"cell_type":"markdown","metadata":{"id":"JfLp2gPn4f_9"},"source":["Handle forewarded emails. For simplicity and lack of time, we'll remove all the forwarded emails here. But if you are interested, you can write a python function to parse these emails and keep only the relevent portions and discard the rest. This can be a bit complicated as one email can be forwarded many times. You can consult the following links which shows how to do it. [link1](https://medium.com/@jubergandharv/email-smart-compose-real-time-assisted-writing-b232191d0681), [link2](https://medium.com/analytics-vidhya/email-smart-compose-assist-in-sentence-completion-b706269da181)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2E55DhAe4f_9"},"outputs":[],"source":["print(emails[\"content\"].iloc[9])"]},{"cell_type":"markdown","metadata":{"id":"r3Hs70Uk4f_9"},"source":["Remove all the emails which contain the string \"Forwarded by\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnkTbnPH4f_9"},"outputs":[],"source":["emails = emails[~emails['content'].str.contains('---------------------- Forwarded by')]"]},{"cell_type":"markdown","metadata":{"id":"XCnU0SYr4f_9"},"source":["Remove all the emails which contain the string \"Original Message\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sxzxhthu4f_9"},"outputs":[],"source":["emails = emails[~emails['content'].str.contains('-----Original Message-----')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FLcwHlm4f_9"},"outputs":[],"source":["emails.shape"]},{"cell_type":"markdown","metadata":{"id":"hfpVKa0u4f_9"},"source":["## Functions to Preprocess Text Data"]},{"cell_type":"markdown","metadata":{"id":"Dha9ReUl4f_9"},"source":["We'll use regular expression extensively for text preprocessing. A full discussion on regular expressions is out of scope for this course. However, I would highyl encourage you to go through these links to know more about them. [link1](https://en.wikipedia.org/wiki/Regular_expression), [link2](https://developers.google.com/edu/python/regular-expressions), [link3](https://docs.python.org/3/library/re.html).\n","\n","We'll mainly use two functions \"re.search(...)\" and \"re.sub(...)\" from the Pyhton \"re\" package. The first is to search for a particular pattern in a string and the second is to substitute a pattern by some other pattern. You can find out more details about these two functions from [here](https://docs.python.org/3/library/re.html)."]},{"cell_type":"markdown","metadata":{"id":"WYQZ_gh94f_9"},"source":["Check if a pattern is present in a string. More about how to form patterns and search function can be found [here](https://docs.python.org/3/library/re.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ivSvUlmj4f_-"},"outputs":[],"source":["# Function to check if a text contains numbers\n","def containNumbers(text):\n","    # check if there is any number is the text\n","    return bool(re.search(r'\\d', text))"]},{"cell_type":"markdown","metadata":{"id":"C3-kSdPq4f_-"},"source":["Function to find out if the string contains special characters. More about forming patterns, compile and search functions [here](https://docs.python.org/3/library/re.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLcffS1X4f_-"},"outputs":[],"source":["# function to check if a text contain special characters\n","def notContainSpecialCharacters(text):\n","    # compile the special characters pattern into regular expressions object\n","    regexp = re.compile(\"\"\"[-+=*@_#`\"$%^&*[\\]\\()<>/\\|}{~:]\"\"\")\n","\n","    # Search for this pattern in the text and return true if it is so\n","    if(regexp.search(text) == None):\n","        return True\n","    else:\n","        return False"]},{"cell_type":"markdown","metadata":{"id":"oWBY_meE4f_-"},"source":["The \"re.sub(...)\" method can be used to substitute a pattern in the string by some other patter. More about this function [here](https://docs.python.org/3/library/re.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XDkTOot4f_-"},"outputs":[],"source":["#Decontraction of text\n","def removeShortForms(text):\n","    \"\"\"\n","    Returns decontracted phrases\n","    \"\"\"\n","    text = re.sub(r\"won't\", \"will not\", text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"yaCawxw64f_-"},"source":["Remove extra spaces from the string. First we'll remove anything other than capital or small letter alphabets using the \"re.sub(...)\" method. Then, we'll split the string into words using using the string \"split(...)\" function and join them using the string \"join(...)\" function. This will effectively remove all the extra spaces within the string. Then, we'll remove the leading and trailing spaces by using the string \"strip(...)\" method. To know more about these string operations, you can go through this [link](https://docs.python.org/3.3/library/stdtypes.html?highlight=split)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AnT-g7wh4f__"},"outputs":[],"source":["def removeSpacesAndConvertToLowercase(text):\n","    # removes anything other than alphabets\n","    text = re.sub('[^A-Za-z]+', ' ', text)\n","\n","    # Remove extra spaces in between, leading and trailaing spaces and covert to lowercase\n","    return ' '.join(text.split()).strip().lower()"]},{"cell_type":"markdown","metadata":{"id":"Ju7kya2N4f__"},"source":["## Preprocess the Email Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvTL6bM94f__"},"outputs":[],"source":["def getPreprocessedSentencesFromEmail(email):\n","    sentences = []\n","\n","    # split the full email on sentence boundary and iterate\n","    for sentence in email.split('.'):\n","\n","        #remove leading and trailing spaces from the sentence\n","        sentence = sentence.strip()\n","\n","        # Find out the number of words in the sentence\n","        no_of_words = len(sentence.split())\n","\n","        if 3 <= no_of_words <= 25 \\\n","            and sentence[0].isupper() and sentence[1:].islower() \\\n","            and not containNumbers(sentence) \\\n","            and notContainSpecialCharacters(sentence):\n","                # Expand the contraced words\n","                sentence = removeShortForms(sentence)\n","\n","                #Split sentence from question mark\n","                for j in re.split('(?<=[?]) +',sentence):\n","\n","                    #Remove extra spaces and append to list\n","                    sentences.append(removeSpacesAndConvertToLowercase(j))\n","\n","    return pd.DataFrame({'sentence':sentences})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSKj6GC84f__"},"outputs":[],"source":["def applyPreprocessingStepsOnAllEmails(emails):\n","    # iterate over all the emails in the dataset\n","    for index in range(emails.shape[0]):\n","        # for the first email a new dataset will be created\n","        if index==0:\n","            result_df = getPreprocessedSentencesFromEmail(emails.content.iloc[0])\n","        else:\n","            # for second email onwards the sentences will be appended\n","            # at the end of the exiting dataframe\n","            result_df = pd.concat([result_df,\n","                            getPreprocessedSentencesFromEmail(emails.content.iloc[index])],\n","                            ignore_index=True)\n","\n","    result_df = result_df.drop_duplicates()\n","    return result_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-6AtPZ94f__"},"outputs":[],"source":["sentences = applyPreprocessingStepsOnAllEmails(emails)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSKntqj14gAA"},"outputs":[],"source":["sentences.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hD-U5skx4gAA"},"outputs":[],"source":["for sentence in sentences.sentence.sample(10, random_state=40):\n","    print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Crx6zjui4gAA"},"outputs":[],"source":["sentences.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58Svu9lP4gAA"},"outputs":[],"source":["#Save dataframe\n","sentences.to_csv('sentences.csv', index=False, index_label=True)"]},{"cell_type":"markdown","source":["# MODEL"],"metadata":{"id":"R7rddfBb9U_M"}},{"cell_type":"code","execution_count":11,"metadata":{"id":"b4b238fd","executionInfo":{"status":"ok","timestamp":1723930865706,"user_tz":-60,"elapsed":3,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"92527256","executionInfo":{"status":"ok","timestamp":1723930866368,"user_tz":-60,"elapsed":320,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}},"outputId":"15dbb180-22c1-4eb0-8c9b-85acc4537d56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            sentence\n","0                               here is our forecast\n","1  traveling to have a business meeting takes the...\n","2   especially if you have to prepare a presentation\n","3  i would suggest holding the business plan meet...\n","4  i would even try and get some honest opinions ..."],"text/html":["\n","  <div id=\"df-17357a3e-2e3d-4863-acaf-b9b9c48bef8f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>here is our forecast</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>traveling to have a business meeting takes the...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>especially if you have to prepare a presentation</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i would suggest holding the business plan meet...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i would even try and get some honest opinions ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17357a3e-2e3d-4863-acaf-b9b9c48bef8f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-17357a3e-2e3d-4863-acaf-b9b9c48bef8f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-17357a3e-2e3d-4863-acaf-b9b9c48bef8f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0f2952fc-dcb2-455b-85e6-aebaf63752b5\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f2952fc-dcb2-455b-85e6-aebaf63752b5')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0f2952fc-dcb2-455b-85e6-aebaf63752b5 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"sentence_df"}},"metadata":{},"execution_count":12}],"source":["sentence_df = pd.read_csv('sentences.csv')\n","sentence_df = sentence_df.dropna()\n","sentence_df.head()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ddb8bdeb","outputId":"b25b860b-6dae-4f93-f777-ad49e14481c6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930867064,"user_tz":-60,"elapsed":379,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of sentence:  152831\n"]}],"source":["sentences = sentence_df.sentence.values\n","print(\"Total number of sentence: \", len(sentences))"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"3fb41a2f","outputId":"9464b809-1fc9-4185-8aff-d74a92b1b235","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930867367,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['here is our forecast',\n","       'traveling to have a business meeting takes the fun out of the trip',\n","       'especially if you have to prepare a presentation',\n","       'i would suggest holding the business plan meetings here then take a trip without any formal business meetings',\n","       'i would even try and get some honest opinions on whether a trip is even desired or necessary',\n","       'too often the presenter speaks and the others are quiet just waiting for their turn',\n","       'the meetings might be better if held in a round table discussion format',\n","       'play golf and rent a ski boat and jet ski is',\n","       'flying somewhere takes too much time',\n","       'plus your thoughts on any changes that need to be made'],\n","      dtype=object)"]},"metadata":{},"execution_count":14}],"source":["sentences[0:10]"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2f74f507","executionInfo":{"status":"ok","timestamp":1723930867367,"user_tz":-60,"elapsed":0,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["sentences = sentences[:30000]"]},{"cell_type":"markdown","metadata":{"id":"58d45479"},"source":["## Tokenization of the sentences"]},{"cell_type":"markdown","metadata":{"id":"decab1dc"},"source":["We'll use keras tokenizer class and its methods to perform tokenization, create vocabulary and the word to number mapping. To know more about tokenizer class, please consult this [link](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"3de0eb52","executionInfo":{"status":"ok","timestamp":1723930868568,"user_tz":-60,"elapsed":232,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"f829d607","executionInfo":{"status":"ok","timestamp":1723930868904,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["test_tokenizer = Tokenizer()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"d1bdfdc4","executionInfo":{"status":"ok","timestamp":1723930868904,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["test_sentences = ['here is our forecast',\n","                  'especially if you have to prepare a presentation']"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"3b6fc0cd","executionInfo":{"status":"ok","timestamp":1723930869224,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["test_tokenizer.fit_on_texts(test_sentences)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"16b631bc","outputId":"71e41018-50e9-47c9-cbbb-262480f1e557","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930869461,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'here': 1,\n"," 'is': 2,\n"," 'our': 3,\n"," 'forecast': 4,\n"," 'especially': 5,\n"," 'if': 6,\n"," 'you': 7,\n"," 'have': 8,\n"," 'to': 9,\n"," 'prepare': 10,\n"," 'a': 11,\n"," 'presentation': 12}"]},"metadata":{},"execution_count":20}],"source":["test_tokenizer.word_index"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"4dbbfd97","outputId":"e3fa0431-6ee6-4739-cf36-cdc601f0d327","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930870421,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{1: 'here',\n"," 2: 'is',\n"," 3: 'our',\n"," 4: 'forecast',\n"," 5: 'especially',\n"," 6: 'if',\n"," 7: 'you',\n"," 8: 'have',\n"," 9: 'to',\n"," 10: 'prepare',\n"," 11: 'a',\n"," 12: 'presentation'}"]},"metadata":{},"execution_count":21}],"source":["test_tokenizer.index_word"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"fe022b37","outputId":"20488aed-f273-4834-a16e-e2b800dfac70","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930870703,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 7, 8, 3, 12]\n"]}],"source":["test_sentence = \"here you have our presentation\"\n","test_token_list = test_tokenizer.texts_to_sequences([test_sentence])[0]\n","print(test_token_list)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"942a6b71","outputId":"00feddce-2d85-4049-ea5d-28f5311d6d69","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930871496,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 7], [1, 7, 8], [1, 7, 8, 3], [1, 7, 8, 3, 12]]\n"]}],"source":["n_grams = []\n","for i in range(1, len(test_token_list)):\n","    n_gram = test_token_list[:i+1]\n","    n_grams.append(n_gram)\n","print(n_grams)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"1cca6d94","executionInfo":{"status":"ok","timestamp":1723930871897,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["tokenizer = Tokenizer()\n","def convertSentencesIntoSeqOfTokens(sentences):\n","    tokenizer.fit_on_texts(sentences)\n","    total_words_in_vocab = len(tokenizer.word_index) + 1\n","\n","    input_sequences = []\n","    for sentence in sentences:\n","        seq_of_tokens = tokenizer.texts_to_sequences([sentence])[0]\n","        for i in range(1, len(seq_of_tokens)):\n","            n_gram = seq_of_tokens[:i+1]\n","            input_sequences.append(n_gram)\n","    return input_sequences, total_words_in_vocab"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"f959dfcd","outputId":"0922f413-0bb8-40d9-b848-d1c62911165a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930874185,"user_tz":-60,"elapsed":1607,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[98, 4],\n"," [98, 4, 41],\n"," [98, 4, 41, 1828],\n"," [2263, 2],\n"," [2263, 2, 17],\n"," [2263, 2, 17, 5],\n"," [2263, 2, 17, 5, 111],\n"," [2263, 2, 17, 5, 111, 114],\n"," [2263, 2, 17, 5, 111, 114, 758],\n"," [2263, 2, 17, 5, 111, 114, 758, 1]]"]},"metadata":{},"execution_count":25}],"source":["input_sequences, total_words_in_vocab = convertSentencesIntoSeqOfTokens(sentences)\n","input_sequences[:10]"]},{"cell_type":"markdown","metadata":{"id":"6dfd7b89"},"source":["## Handle variable sentence lengths by padding"]},{"cell_type":"markdown","metadata":{"id":"afe8b9b1"},"source":["We'll use the Keras \"pad_sequences\" function to pad smaller sequences. To know more about this function, please go through this [link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ca33bbe8","executionInfo":{"status":"ok","timestamp":1723930875092,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"93ba1827","executionInfo":{"status":"ok","timestamp":1723930875436,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["test_sequences = [[2025, 2], [2025, 2, 16], [2025, 2, 16, 6],\n","                  [2025, 2, 16, 6, 135], [2025, 2, 16, 6, 135, 119]]"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"bd2f2539","outputId":"ccd806f4-bb39-48cf-8dc1-7408b10d96e4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930875748,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   0,    0,    0,    0, 2025,    2],\n","       [   0,    0,    0, 2025,    2,   16],\n","       [   0,    0, 2025,    2,   16,    6],\n","       [   0, 2025,    2,   16,    6,  135],\n","       [2025,    2,   16,    6,  135,  119]], dtype=int32)"]},"metadata":{},"execution_count":28}],"source":["pad_sequences(test_sequences, maxlen=6, padding='pre')"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"8c2b812a","executionInfo":{"status":"ok","timestamp":1723930876157,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["def generateSameLengthSentencesByPadding(sequences):\n","    # Find length of the longest sequence\n","    max_seq_len = max([len(x) for x in sequences])\n","\n","    # Pad the senquences\n","    padded_sequences = np.array(pad_sequences(sequences, maxlen=max_seq_len, padding='pre'))\n","\n","    # Return padded sequences and the max length\n","    return padded_sequences, max_seq_len"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"465aa747","outputId":"cdedf91b-7300-41f6-a1c6-5901b067816e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930878119,"user_tz":-60,"elapsed":1297,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(319492, 28)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","        0,  0,  0,  0,  0,  0,  0,  0,  0, 98,  4], dtype=int32)"]},"metadata":{},"execution_count":30}],"source":["padded_sequences, max_seq_len = generateSameLengthSentencesByPadding(input_sequences)\n","print(padded_sequences.shape)\n","padded_sequences[0]\n"]},{"cell_type":"markdown","metadata":{"id":"078b37f7"},"source":["## Generate predictors and labels for training"]},{"cell_type":"markdown","metadata":{"id":"be045f48"},"source":["We are importing keras utils here, this will be needed to convert the labels to one-hot encoded vectors. We'll use the the function \"to_categorical\" from this library to do this. To know more about this function, please check out this [link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"d79a5c4c","executionInfo":{"status":"ok","timestamp":1723930878119,"user_tz":-60,"elapsed":1,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["import tensorflow.keras.utils as ku"]},{"cell_type":"markdown","metadata":{"id":"85abc484"},"source":["We'll use array slicing techniques to retrieve the inputs and the labels. To know more about how indexing into a numpy array is done, please go through the following resources. [link1](https://towardsdatascience.com/slicing-numpy-arrays-like-a-ninja-e4910670ceb0), [link2](https://www.tutorialspoint.com/numpy/numpy_indexing_and_slicing.htm)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"2f9bc390","executionInfo":{"status":"ok","timestamp":1723930879399,"user_tz":-60,"elapsed":227,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["def generatePredictorsAndLabels(padded_sequences):\n","    inputs, label = padded_sequences[:,:-1], padded_sequences[:,-1]\n","    label = ku.to_categorical(label, num_classes = total_words_in_vocab)\n","    return inputs, label"]},{"cell_type":"markdown","source":[],"metadata":{"id":"DJ04tFexLjrM"}},{"cell_type":"code","execution_count":33,"metadata":{"id":"c3bf8c7a","executionInfo":{"status":"ok","timestamp":1723930881759,"user_tz":-60,"elapsed":1077,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["inputs, label = generatePredictorsAndLabels(padded_sequences)"]},{"cell_type":"markdown","metadata":{"id":"3ccafba3"},"source":["## Create and train the model"]},{"cell_type":"markdown","metadata":{"id":"7da87dd7"},"source":["Import Sequential model from keras and Embedding, LSTM, Dense and Dropout layers from Keras. To know more about them, consult these links. [sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential), [embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n","\n","To know more about regularization, overfitting and dropout strategy, please consult this [link](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"9f3d5e2e","executionInfo":{"status":"ok","timestamp":1723930884126,"user_tz":-60,"elapsed":244,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"4a67e545","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723930886104,"user_tz":-60,"elapsed":229,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}},"outputId":"11beeb8c-dace-4e27-d3fa-85a598955f3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_length: 27\n","total_words_in_vocab: 16027\n"]}],"source":["input_length = max_seq_len - 1 # because the last was used as label\n","print(f\"input_length: {input_length}\")\n","print(f\"total_words_in_vocab: {total_words_in_vocab}\")"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"398b2e6f","executionInfo":{"status":"ok","timestamp":1723930890609,"user_tz":-60,"elapsed":1501,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(input_dim=total_words_in_vocab, output_dim=10)) #Turns positive integers (indexes) into dense vectors of fixed size\n","model.add(LSTM(100))\n","model.add(Dropout(0.1))\n","model.add(Dense(total_words_in_vocab, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"f3b2bfaa"},"source":["Compile the model by specifying the loss function and the optimizer which the model will use during training. To know more about different loss functions and optimizers, go through these links. [link1](https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c), [link2](https://towardsdatascience.com/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a), [link3](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"2ed028b3","outputId":"315007f8-52af-490f-b0a6-72fc09796f0f","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1723930502431,"user_tz":-60,"elapsed":2,"user":{"displayName":"Nicolas Montaño","userId":"09446804150664905730"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdaa5870"},"outputs":[],"source":["tf.debugging.set_log_device_placement(True)\n","\n","try:\n","  # Specify an invalid GPU device\n","  with tf.device('/device:GPU:0'):\n","    history = model.fit(inputs, label, epochs=100, verbose=1)\n","except RuntimeError as e:\n","  print(e)"]},{"cell_type":"markdown","metadata":{"id":"83bc2b58"},"source":["Once you are done with the training, you can save your model. To know more about this, follow this [link](https://www.tensorflow.org/guide/keras/save_and_serialize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73f4c676"},"outputs":[],"source":["model.save('lstm_text_autocomplete')"]},{"cell_type":"markdown","metadata":{"id":"01183ecc"},"source":["## Generate autocomplete suggestions using the trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae90ba35"},"outputs":[],"source":["from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd8905fe"},"outputs":[],"source":["model = load_model('lstm_text_autocomplete')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d92697ee"},"outputs":[],"source":["def generate_autocomplete_suggestions(seed_sentence, no_of_next_words,\n","                                      model, max_sequence_len):\n","    for _ in range(no_of_next_words):\n","        sequence = tokenizer.texts_to_sequences([seed_sentence])[0]\n","\n","        padded_sequence = pad_sequences([sequence],\n","                                        maxlen=max_seq_len-1,\n","                                        padding='pre')\n","\n","        predictions = model.predict(padded_sequence, verbose=0)\n","\n","        predicted_label = np.argmax(predictions, axis=1)[0]\n","\n","        next_word = tokenizer.index_word[predicted_label]\n","\n","        seed_sentence += \" \"+ next_word\n","\n","    return seed_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49330b01"},"outputs":[],"source":["print (generate_autocomplete_suggestions(\"In response to your earlier email\", 10,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"I am happy to\", 10,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"What is the status\", 3,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"Here is the data\", 3,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"Thank you very much\", 4,\n","                                         model, max_seq_len))\n","\n","print (generate_autocomplete_suggestions(\"I got your email\", 17,\n","                                         model, max_seq_len))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}